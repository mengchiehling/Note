{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install\n",
    "\n",
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template\n",
    "\n",
    "Data preprocessing is done on the side of traditional python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def PCA_transformation(X_train, X_test, categorial_columns):\n",
    "\n",
    "    pca = PCA(n_components=24) # In this case the explained_variance_ratio_ exceed 0.99999\n",
    "\n",
    "    X = pd.concat((X_train, X_test), ignore_index=True)\n",
    "\n",
    "    X_apply = X.drop(labels=categorial_columns, axis=1)\n",
    "\n",
    "    X_pca = pca.fit_transform(X_apply)\n",
    "\n",
    "    X_pca_std = X_pca.std(axis = 0)\n",
    "\n",
    "    X_pca_norm = X_pca/X_pca_std\n",
    "\n",
    "    df_pca = pd.DataFrame(data=X_pca_norm, columns=[\"Feature_{:02d}\".format(i) for i in range(24)])\n",
    "\n",
    "    df_combined = pd.concat([X[categorial_columns], df_pca], axis=1)\n",
    "\n",
    "    return df_combined.iloc[:len(X_train)], df_combined.iloc[len(X_train):]\n",
    "\n",
    "\"\"\"\n",
    "Start loading the data\n",
    "\"\"\"\n",
    "X_train = pd.read_csv(\"../python/KNN_minkowski_features.csv\", index_col=0)\n",
    "y_train = X_train[X_train.columns[-1]]\n",
    "X_train.drop(X_train.columns[-1], axis=1, inplace=True)\n",
    "\n",
    "X_test = pd.read_csv(\"../python/KNN_minkowski_features_test.csv\", index_col=0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Finish loading the data\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "PCA Transformation to reduce the dimensionality\n",
    "\"\"\"\n",
    "X_train_pca, X_test_pca = PCA_transformation(X_train, X_test, ['shop_id', 'item_id', 'cats'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Coding on item_id, shop_id, and cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. item_id\n",
    "\n",
    "index_cols = ['shop_id', 'item_id', 'cats']\n",
    "\n",
    "all_data = X_train_pca.copy()\n",
    "all_data = pd.concat((X_train_pca, y_train), axis=1, join='inner')\n",
    "\n",
    "all_data.sort_values(index_cols,inplace=True)\n",
    "\n",
    "all_data['item_target_enc'] = all_data.groupby('item_id')['target'].transform('mean')\n",
    "all_data['item_target_count'] = all_data.groupby('item_id')['target'].transform('count')\n",
    "all_data['item_target_enc_smooth'] = (all_data['item_target_enc'] * all_data['item_target_count'] + 0.3343 * 100) \\\n",
    "/(all_data['item_target_count'] + 100.0)\n",
    "\n",
    "all_data_item_encode = all_data[['shop_id', 'item_id', 'cats', 'item_target_enc_smooth']]\n",
    "\n",
    "#2. shop_id\n",
    "\n",
    "all_data['shop_target_enc'] = all_data.groupby('shop_id')['target'].transform('mean')\n",
    "all_data['shop_target_count'] = all_data.groupby('shop_id')['target'].transform('count')\n",
    "all_data['shop_target_enc_smooth'] = (all_data['shop_target_enc'] * all_data['shop_target_count'] + 0.3343 * 100) \\\n",
    "/(all_data['shop_target_count'] + 100.0)\n",
    "\n",
    "all_data_shop_encode = all_data[['shop_id', 'item_id', 'cats', 'shop_target_enc_smooth']]\n",
    "\n",
    "#3. cat\n",
    "\n",
    "all_data['cat_target_enc'] = all_data.groupby('cats')['target'].transform('mean')\n",
    "all_data['cat_target_count'] = all_data.groupby('cats')['target'].transform('count')\n",
    "all_data['cat_target_enc_smooth'] = (all_data['cat_target_enc'] * all_data['cat_target_count'] + 0.3343 * 100) \\\n",
    "/(all_data['cat_target_count'] + 100.0)\n",
    "\n",
    "all_data_cat_encode = all_data[['shop_id', 'item_id', 'cats', 'cat_target_enc_smooth']]\n",
    "\n",
    "X_train_pca = X_train_pca.merge(all_data_item_encode, on=index_cols)\n",
    "X_train_pca = X_train_pca.merge(all_data_shop_encode, on=index_cols)\n",
    "X_train_pca = X_train_pca.merge(all_data_cat_encode, on=index_cols)\n",
    "\n",
    "X_train_pca.drop(labels=index_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca = X_test_pca.merge(all_data_item_encode, on=index_cols)\n",
    "X_test_pca = X_test_pca.merge(all_data_shop_encode, on=index_cols)\n",
    "X_test_pca = X_test_pca.merge(all_data_cat_encode, on=index_cols)\n",
    "\n",
    "X_test_pca.drop(labels=index_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_pyspark = pd.concat((X_train_pca, y_train), axis=1, join='inner')\n",
    "\n",
    "\"\"\"\n",
    "Transform the pandas dataframe to pyspark dataframe\n",
    "\"\"\"\n",
    "\n",
    "X_train_pyspark = spark.createDataFrame(X_train_pca_pyspark)\n",
    "X_test_pyspark = spark.createDataFrame(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "stages = []\n",
    "\n",
    "numericCol = [feature for feature in X_train_pca.columns]\n",
    "featureAssembler = VectorAssembler(inputCols=numericCol, outputCol='features')\n",
    "\n",
    "stages += [featureAssembler]\n",
    "\n",
    "# class pyspark.ml.regression.GBTRegressor(self, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",\n",
    "#                                          maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, \n",
    "#                                          maxMemoryInMB=256, cacheNodeIds=False, subsamplingRate=1.0, \n",
    "#                                          checkpointInterval=10, lossType=\"squared\", maxIter=20, stepSize=0.1, \n",
    "#                                          seed=None, impurity=\"variance\")\n",
    "\n",
    "\"\"\"\n",
    "Train a GBT model\n",
    "\"\"\"\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol='target')\n",
    "\n",
    "\"\"\"\n",
    "Chain featureAssembler and GBT in a Pipeline\n",
    "\"\"\"\n",
    "pipeline = Pipeline(stages=[featureAssembler, gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "GridSearch\n",
    "\"\"\"\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxDepth, np.arange(3, 12, 2)) \\\n",
    "                              .addGrid(gbt.minInstancesPerNode, np.arange(1, 12, 5)) \\\n",
    "                              .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='target', metricName='mse')\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "gbt_cvModel = crossval.fit(X_train_pyspark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
